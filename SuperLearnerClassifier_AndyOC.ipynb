{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: The Super Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Project completed using a Conda virtual env.\n",
    "\n",
    "   - [Anaconda3](https://conda.io/docs/user-guide/install/index.html)\n",
    "\n",
    "   - [Manageing Conda Environment](https://conda.io/docs/user-guide/tasks/manage-environments.html)\n",
    "\n",
    "To setup an identical environment to the one used.\n",
    "\n",
    "    conda env create -f environment.yml\n",
    "\n",
    "Or to build your own.\n",
    "\n",
    "    conda create --name ENV_NAME --file spec-file.txt\n",
    "    \n",
    "both files can be found in the env folder.\n",
    "\n",
    "\n",
    "Unofficial Jupyter extensions were used to make Jupyter Notebook a more indepth IDE. \n",
    "They aren't included in spec-file.txt or environment.yml but they are very cool.\n",
    "\n",
    "   - [nbextensions](http://jupyter-contrib-nbextensions.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unittest\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "from functools import reduce\n",
    "from inspect import getmembers\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from itertools import chain\n",
    "\n",
    "from IPython.display import display, HTML, Image\n",
    "from IPython.html.services.config import ConfigManager\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "#%qtconsole\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and PKLS\n",
    "In this notebook I use joblib to PKL classifiers, it saves re-training and redoing GridSearches. It's also a safety net against loss of results. To keep things organised, they are kept in the pkl folder. PKLS cannot however be included in the submission as they are too large. Some searches were kept as they harbour the searches best model and the results across the gridsearch, these give valuable infromation to reflect on the model and understand what type of parameters and base learners suit the MNIST set. \n",
    "\n",
    "#### IMPORTANT\n",
    "To create new set of pkls, simply delete or move or rename the old ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([3, 6, 8, 3, 6, 8]), array([1, 2, 4, 5])),\n",
       " (array([0, 0, 7, 1, 4, 4, 4]), array([3, 6, 9])),\n",
       " (array([5, 5, 1, 2, 2, 5, 5]), array([0, 7, 8]))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkls = os.path.join(os.getcwd(),'pkls')\n",
    "tuned_bases = os.path.join(pkls, 'tuned_bases')\n",
    "super_learners = os.path.join(pkls, 'super_learners')\n",
    "searches = os.path.join(pkls, 'searches')\n",
    "\n",
    "\n",
    "#Some useful functions.\n",
    "\n",
    "def name(class_str):\n",
    "    '''A small function for parsing class names'''\n",
    "    class_str = type(class_str)\n",
    "    class_str = str(class_str).split('.')[-1]\n",
    "    alphas = [c for c in list(class_str) if c.isalnum()]\n",
    "    return ''.join(alphas)\n",
    "\n",
    "\n",
    "\n",
    "#This function should exist in SKLearn!\n",
    "def bootstrapped_KFold(X, y=None, n_splits=3):\n",
    "    '''Returns a generatir for KFold with resampling with replacement'''\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    bootstrapped_cv = ((resample(train), test) for train, test in kf.split(X, y))\n",
    "    return bootstrapped_cv\n",
    "\n",
    "list(bootstrapped_KFold(np.arange(10), n_splits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Diversity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pairwise diversity measures.\n",
    "ref : Kuncheva et al. 2003\n",
    "Similar implementation @ http://brew.readthedocs.io/en/latest/\n",
    "brew.diversity.paired.py has a Python2 implementation for its ensemble API.\n",
    "Key differences:\n",
    "Brew produces an aggregation of each metrics for the ensemble.\n",
    "Brew is faster. \n",
    "\n",
    "My functions produce pairwise metrics for each pair of base estimators;\n",
    "allowing for more indepth analysis. These functions are more fit for\n",
    "purpose in this project.\n",
    "'''\n",
    "\n",
    "def coefs(y_pred_1, y_pred_2, y):\n",
    "    A = np.equal(y_pred_1, y)\n",
    "    B = np.equal(y_pred_2, y)\n",
    "    d,c,b,a = confusion_matrix(A,B).ravel()\n",
    "    return a,b,c,d\n",
    "\n",
    "def disagree(y_pred_1, y_pred_2, y):\n",
    "    a,b,c,d = coefs(y_pred_1, y_pred_2, y)\n",
    "    return (b + c) / sum((a,b,c,d))\n",
    "\n",
    "def q_stat(y_pred_1, y_pred_2, y):\n",
    "    a,b,c,d = coefs(y_pred_1, y_pred_2, y)\n",
    "    return (a*d - b*c)/(a*d + b*c)\n",
    "\n",
    "def double_fault(y_pred_1, y_pred_2, y):\n",
    "    a,b,c,d = coefs(y_pred_1, y_pred_2, y)\n",
    "    return d / sum((a,b,c,d))\n",
    "\n",
    "\n",
    "'''Ensemble expertise metrics; I made these to get more diagnostic\n",
    "infromation on how well the ensemble covers data in the test set.'''\n",
    "def unanimous_agreement(ensemble, X, y):\n",
    "    '''Measures the proportion of the data where bases unanimously agree.\n",
    "    '''\n",
    "    accum = np.equal(y,y)\n",
    "    for base in ensemble:\n",
    "        data = np.equal(base.predict(X), y)\n",
    "        accum = (data*accum)\n",
    "    return np.sum(accum) / len(accum)\n",
    "        \n",
    "def missing_expertise(ensemble, X, y):\n",
    "    '''Measures the proportion of the data where no member has produced a\n",
    "    correct prediction.\n",
    "    '''\n",
    "    accum = np.equal(y,~y) #Falses\n",
    "    for base in ensemble:\n",
    "        data = np.equal(base.predict(X), y) #True for correct predicitons\n",
    "        accum = np.logical_or(data, accum) #True for any correct prediction.\n",
    "    return(np.sum(~accum)) / len(accum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier\n",
    "\n",
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/19e9/c732082706f39d2ba12845851309714db135.pdf) but the stacked ensemble idea has been around for a long time. \n",
    "\n",
    "Figure 1 shows a flow diagram of the Super Learner process (this is from (van der Laan et al, 2007) and the process is also described in the COMP47590 lecture \"[COMP47590 2017-2018 L04 Supervised Learning Ensembles 3](https://www.dropbox.com/s/1ksx94nxtuyn4l8/COMP47590%202017-2018%20L04%20Supervised%20Learning%20Ensembles%203.pdf?raw=1)\"). The base classifiers are trained and their outputs are combined along with the training dataset labels into a training set for the stack layer classifier. To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process (described as V-fold in Figure 1). To further add variety to the base estimators a bootstrapping selection (as is used in the bagging ensemble approach).\n",
    " \n",
    "![Super Learner Process Flow](SuperLearnerProcessFlow.png \"Logo Title Text 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class\n",
    "\n",
    "    Some documentation that was adhered to when creating the SuperLearnerClassifier.\n",
    "\n",
    "   - [Sklearn contributors guidelines](http://scikit-learn.org/stable/developers/contributing.html)\n",
    "   \n",
    "    \n",
    "   - [PEP8 style for SKLearn consistancy](https://www.python.org/dev/peps/pep-0008/)\n",
    "   \n",
    "   \n",
    "   - [NumPy/SciPy docstring conventions](https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt)\n",
    "   \n",
    "TLDR: \n",
    "\n",
    "Docstring Linelengths >=75 chars \n",
    "\n",
    "Code line lengths >= 80\n",
    "\n",
    "Docstring sections e.g. Short Summary, parameters, raises, returns etc.\n",
    "\n",
    "Don't include private/class methods attribute in docs if not part of public API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models in its base \n",
    "    layer and an aggregation model at the stacked layer. K-fold cross\n",
    "    validation is used to generate training data for the stacked layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    BaseEstimator : \n",
    "                     Base class for all estimators in scikit-learn\n",
    "    ClassifierMixin : Mixin class for all classifiers in scikit-learn.\n",
    "\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    __allowed_bases : tuple of class names\n",
    "                      permitted base layer estimators.\n",
    "    __allowed_stacked : tuple of class names\n",
    "                        permitted stacked layer estimators.\n",
    "    estimators : list of estimators\n",
    "                 Instances of base estimators.\n",
    "    stacked_estimator: estimator\n",
    "                       estimator used in the stack layer.                       \n",
    "    probability : bool, optional\n",
    "                  True/False to use probability/class labels in stacked\n",
    "                  dataset.    \n",
    "    keep_original : bool, optional\n",
    "                    include/exclude original input data to stacked layer                    \n",
    "    probabilities : dict\n",
    "                    stores whether a base can output a probability label.\n",
    "    is_fitted_ : bool\n",
    "                 stores whether classifier has been fitted.\n",
    "    \n",
    "    Methods\n",
    "    -------    \n",
    "    fit(X, y)\n",
    "        Fit the model according to the given training data.\n",
    "    get_params([deep])\n",
    "        Get parameters for this estimator.\n",
    "    predict(X)\n",
    "        Predict class labels for samples in X.\n",
    "    predict_proba(X)\n",
    "        Probability estimates.\n",
    "    set_params(**params)\n",
    "        Set the parameters of this estimator.    \n",
    "    todo: score(X, y)\n",
    "    todo: add sample_weight for fit,score\n",
    "    \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    1. ensures conformity to allowed bases.\n",
    "    2. ensures conformity to allowed stacked estimators.\n",
    "    3. To pass GridSearch estimators; use their best_estimator_ attribute.\n",
    "    4. handles GridSearch type estimators and checks for their conformity.\n",
    "    5. allows for use of n_jobs where appropriate.\n",
    "    6. No Pandas dependency data handles using NumPy/SKLearn\n",
    "    7. param and attribute names appropriately aligned with SKLearn.\n",
    "    8. Can select which estimators give probability bases labels.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "         Super Learner. Statistical Applications in Genetics \n",
    "         and Molecular Biology, 6(1) \n",
    "         doi:10.2202/1544-6115.1309\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = SuperLearnerClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "    \"\"\"\n",
    "        \n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, estimators=None, stacked_estimator=None,\n",
    "                 cv=10, probability=False, keep_original=False):       \n",
    "        \"\"\"Setup a SuperLearner classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimators : list of estimators, int optional (default=None)        \n",
    "                     A list of the instances of base estimators. If None\n",
    "                     estimators default to a decision tree and logistic\n",
    "                     regression. If an int n where 0<n<14; list equals the \n",
    "                     first n estimators in allowed_bases.\n",
    "        stacked_estimator : estimator, optional (default=None)\n",
    "                            Estimator to be used on the stack layer. if \n",
    "                            None estimator defauls to a decision tree.\n",
    "        cv : int, optional (default=10)\n",
    "             folds of cross validation used to create stacked layer.\n",
    "        probability : bool, optional (default=False)\n",
    "                      if True, probability estimations for all possible\n",
    "                      learners are used to generate stacked dataset instead\n",
    "                      of class labels. False, class label predictions are\n",
    "                      added to stacked dataset.\n",
    "        keep_original : bool, optional (default=False)\n",
    "                        True/False to include/exclude raw input data in the\n",
    "                        stacked training set.        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        Notes\n",
    "        -------\n",
    "        1. No logic, just attribute assignment. Consistent with SKLearn.\n",
    "\n",
    "        \"\"\"\n",
    "        self.__allowed_bases = (\n",
    "            MLPClassifier,\n",
    "            KNeighborsClassifier,\n",
    "            NearestCentroid,\n",
    "            #SVC, #probability=True\n",
    "            ExtraTreesClassifier,\n",
    "            RandomForestClassifier,\n",
    "            AdaBoostClassifier,\n",
    "            GradientBoostingClassifier,\n",
    "            GaussianProcessClassifier,\n",
    "            GaussianNB,\n",
    "            BernoulliNB,\n",
    "            MultinomialNB,\n",
    "            LogisticRegression,\n",
    "            #SGDClassifier,\n",
    "            #PassiveAggressiveClassifier,\n",
    "            DecisionTreeClassifier,\n",
    "            LinearDiscriminantAnalysis,\n",
    "            QuadraticDiscriminantAnalysis\n",
    "        )\n",
    "        \n",
    "        self.__allowed_stacked = (\n",
    "            #MLPClassifier,\n",
    "            KNeighborsClassifier,\n",
    "            NearestCentroid,\n",
    "            #SVC, #predict_proba if probability=True\n",
    "            ExtraTreesClassifier,\n",
    "            RandomForestClassifier,\n",
    "            AdaBoostClassifier,\n",
    "            GradientBoostingClassifier,\n",
    "            GaussianProcessClassifier,\n",
    "            GaussianNB,\n",
    "            BernoulliNB,\n",
    "            MultinomialNB,\n",
    "            LogisticRegression,\n",
    "            #SGDClassifier,\n",
    "            #PassiveAggressiveClassifier,\n",
    "            DecisionTreeClassifier,\n",
    "            LinearDiscriminantAnalysis,\n",
    "            QuadraticDiscriminantAnalysis\n",
    "        )\n",
    "        \n",
    "        self.estimators = estimators\n",
    "        self.stacked_estimator = stacked_estimator\n",
    "        self.cv = cv    \n",
    "        self.probability = probability\n",
    "        self.keep_original = keep_original\n",
    "        \n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \n",
    "        \"\"\"        \n",
    "        X, y = check_X_y(X, y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self._setup_base_estimators()\n",
    "        self._setup_stacked_estimators()\n",
    "        self._setup_probabilities()\n",
    "        stacked_set = self.__setup_stacked_set(X,y, purpose='train')\n",
    "        for clf in self.estimators:\n",
    "            clf.fit(X, y)\n",
    "        try:\n",
    "            self.stacked_estimator.fit(stacked_set, y, n_jobs=n_jobs)\n",
    "            '''todo: what error is thrown if no n_jobs param exists?'''\n",
    "        except:\n",
    "            self.stacked_estimator.fit(stacked_set, y)\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        y : not used\n",
    "       \n",
    "        Raises\n",
    "        ------\n",
    "        NotFittedError\n",
    "            if 'fit' has not been executed before 'predict' is called.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples.\n",
    "        \n",
    "        Notes\n",
    "        ------\n",
    "        1. Predicts outcome according to each base classifier\n",
    "        2. feeds those predictions (and raw data if keep_original=True) to \n",
    "           stacked estimator.\n",
    "        \n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, ['is_fitted_'])\n",
    "        stacked_set = self.__setup_stacked_set(X,y, purpose='predict')\n",
    "        p = self.stacked_estimator.predict(stacked_set)\n",
    "        return p \n",
    "        #except NotFittedError as e:\n",
    "            #print('Error in SuperLearnerClassifier',repr(e))\n",
    "            #raise NotFittedError('Call fit before prediction')\n",
    "    \n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        NotFittedError\n",
    "            if 'fit' has not been executed before 'predict_proba' is called.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, ['is_fitted_'])\n",
    "        stacked_set = self.__setup_stacked_set(X,y, purpose='predict')\n",
    "        p = self.stacked_estimator.predict_proba(stacked_set)\n",
    "        return p\n",
    "    \n",
    "    \n",
    "    def diversity(self, X, y):\n",
    "        '''Returns pairwise diversity metrics for all base pairs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pairwise : dict\n",
    "                   dictionary diversity metrics for each pair of\n",
    "                   estimators in self.estimators\n",
    "        '''\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_is_fitted(self, ['is_fitted_'])\n",
    "        all_pairs = list(combinations(self.estimators,2))\n",
    "        \n",
    "        def pair(*args):\n",
    "            return tuple([name(arg) for arg in args])\n",
    "        \n",
    "        pairwise = {'Pairs' : [pair(one, two)\\\n",
    "                              for one, two in all_pairs],       \n",
    "            'Matthews Coef.'  : [matthews_corrcoef(one.predict(X),\n",
    "                                                   two.predict(X)) \\\n",
    "                                 for one,two in all_pairs],\n",
    "            'Disagree': [disagree(one.predict(X),\n",
    "                                  two.predict(X),\n",
    "                                  y) \\\n",
    "                        for one, two in all_pairs],\n",
    "            'Q': [q_stat(one.predict(X),\n",
    "                         two.predict(X),\n",
    "                         y) \\\n",
    "                  for one, two in all_pairs],\n",
    "            'Double Fault':[double_fault(one.predict(X),\n",
    "                                         two.predict(X),\n",
    "                                         y) \\\n",
    "                            for one, two in all_pairs]\n",
    "           }\n",
    "        return pairwise\n",
    "        \n",
    "    \n",
    "    def score_bases(self, X, y):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pairwise : dict\n",
    "                   dictionary accuracy for for each base classifier\n",
    "                   estimators in self.estimators\n",
    "        '''\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_is_fitted(self, ['is_fitted_'])\n",
    "        scores = {'Estimator': [name(base) \\\n",
    "                               for base in self.estimators],\n",
    "                  'Accuracy' : [accuracy_score(y, base.predict(X))\\\n",
    "                               for base in self.estimators],\n",
    "             }\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def coverage(self, X, y):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        '''\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_is_fitted(self, ['is_fitted_'])\n",
    "        scores = {'Unanimous_Agreement' : unanimous_agreement(self.estimators,\n",
    "                                                          X,\n",
    "                                                          y),\n",
    "                  'Missing_Expertise' : missing_expertise(self.estimators,\n",
    "                                                          X,\n",
    "                                                          y)\n",
    "                 }\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def _setup_base_estimators(self):\n",
    "        '''runs setup logic for estimators attribute.\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "             if invalid int is used\n",
    "             or forbidden base estimator is used.\n",
    "        TypeError\n",
    "             if an invalid type for estimators is used. e.g. dict.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        if self.estimators == None: \n",
    "            self.estimators = [LogisticRegression(),\n",
    "                               DecisionTreeClassifier()]  \n",
    "        elif isinstance(self.estimators, int):\n",
    "            if 0 < self.estimators < len(self.__allowed_bases): \n",
    "                subset = self.__allowed_bases[0:self.estimators]\n",
    "                self.estimators = [clf() for clf in subset]\n",
    "            else: \n",
    "                raise ValueError('Set classifiers to an int between 0 and',\n",
    "                                 len(self.__allowed_bases))\n",
    "        elif any([self._is_valid_base(clf) for clf in self.estimators]):\n",
    "            if all([self._is_valid_base(clf) for clf in self.estimators]): \n",
    "                self.estimators = list(self.estimators)\n",
    "            else: \n",
    "                raise ValueError('Invalid base estimator used.',\n",
    "                                 'Use any of--->', self.__allowed_bases)\n",
    "        else: raise TypeError('Unexpected type--->',type(self.estimators),\\\n",
    "                              '. Use--->int or list of classifiers')\n",
    "            \n",
    "    \n",
    "    \n",
    "    def _setup_stacked_estimators(self):\n",
    "        '''runs setup logic for stacked estimators attribute\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "\n",
    "        TypeError\n",
    "             if an invalid estimators is used. e.g. random forest regressor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        if self.stacked_estimator == None:\n",
    "            self.stacked_estimator = DecisionTreeClassifier()\n",
    "        elif not isinstance(self.stacked_estimator, self.__allowed_stacked): \n",
    "            raise TypeError('Invalid stacked estimator used--->',\\\n",
    "                            type(self.stacked_estimator),\\\n",
    "                            'Use any of--->',\\\n",
    "                            self.__allowed_stacked )\n",
    "     \n",
    "    \n",
    "    \n",
    "    def _setup_probabilities(self):\n",
    "        '''runs setup logic for probabilities attribute\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        def has_proba(clf):\n",
    "                return callable(getattr(clf, 'predict_proba', None)) \n",
    "        if self.probability:\n",
    "            self.__probabilities = {clf:has_proba(clf) for clf in self.estimators}\n",
    "        else:\n",
    "            self.__probabilities = {clf:False for clf in self.estimators}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _is_valid_base(self, clf):\n",
    "        \"\"\"checks if clf is a permitted base estimator\n",
    "        ----------\n",
    "        clf : estimator\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        permitted : bool\n",
    "                    True if classifier is valid, false otherwise.\n",
    "        \"\"\"\n",
    "        is_allowed = isinstance(clf, self.__allowed_bases)\n",
    "        is_grid_search = (hasattr(clf, 'best_estimator_') \\\n",
    "                          and isinstance(clf.best_estimator_, \n",
    "                                         self.__allowed_bases))\n",
    "        permitted = is_allowed or is_grid_search\n",
    "        return permitted\n",
    "\n",
    "\n",
    "    \n",
    "    def __setup_stacked_set(self, X, y=None, purpose='predict'):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        y : None, not used.\n",
    "            Present for consitancy with other methods.\n",
    "        purpose : str, optional (default=predict)\n",
    "                  'predict' is stacked set is designed for prediciton.\n",
    "                  'train' is stacked set is required for training/fitting.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        stacked_set : array of shape = [n_samples, n_labels].\n",
    "                      dataset used for stacked layer, in training and \n",
    "                      prediction\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        no_rows, no_cols = X.shape\n",
    "        if not self.keep_original: no_cols = 0\n",
    "        i = no_cols\n",
    "        size_method = self.__size_method(purpose)\n",
    "        extra_cols = reduce(lambda x,y:x+y, \n",
    "                            map(lambda clf:size_method[clf][0],\n",
    "                                                self.estimators))\n",
    "        stacked_set = np.zeros((no_rows, no_cols + extra_cols))\n",
    "        \n",
    "        if purpose == 'train':                \n",
    "            for clf in self.estimators:\n",
    "                size, method = size_method[clf]\n",
    "                \n",
    "                #cross_val_predict with bootstrapping\n",
    "                stacked_set[:,i:i+size] = cross_val_predict(\n",
    "                    estimator=clf,\n",
    "                    X=X, \n",
    "                    y=y, \n",
    "                    cv=bootstrapped_KFold(X, n_splits=self.cv),\n",
    "                    method=method).reshape(no_rows, size)\n",
    "                i += size\n",
    "        \n",
    "        elif purpose == 'predict':        \n",
    "            for clf in self.estimators:\n",
    "                size, method_func = size_method[clf]\n",
    "                stacked_set[:,i:i+size] = method_func(\n",
    "                    X).reshape(no_rows, size)\n",
    "                i += size\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('set purpose to either `train` or `predict`')\n",
    "        \n",
    "        if self.keep_original: stacked_set[0:no_rows, 0:no_cols] = X            \n",
    "        return stacked_set\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __size_method(self, purpose):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        size_method : dictionary with key: clf and value tuple(size, method)\n",
    "        \"\"\"\n",
    "        size = self._size_of_output\n",
    "        method = self._method_of_output\n",
    "        size_method = {clf:(size(clf), \n",
    "                            method(clf,purpose)) for clf in self.estimators}\n",
    "        return size_method\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _size_of_output(self, clf):\n",
    "        \"\"\"returns size of array or prediciton produced by clf.\n",
    "        Parameters\n",
    "        ----------\n",
    "        clf : estimator\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        size : int\n",
    "                if classifier uses probaility output labels, size is \n",
    "                number of labels, otherwise 1.\n",
    "        \"\"\"\n",
    "        if self.probability and self.__probabilities[clf]:\n",
    "            return len(self.classes_)\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _method_of_output(self, clf, purpose):\n",
    "        \"\"\"returns size of array or prediciton produced by clf.\n",
    "        Parameters\n",
    "        ----------\n",
    "        clf : estimator\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        method : str, function_object\n",
    "                 method string if purpose is training\n",
    "                 function object if purpose is prediction.\n",
    "        \"\"\"\n",
    "        try_method = getattr(clf, 'predict_proba', clf.predict)\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            try_string= 'predict_proba'  \n",
    "        else:\n",
    "            try_string = 'predict'\n",
    "                    \n",
    "        results = {'train':['predict', try_string],\n",
    "                  'predict':[clf.predict, try_method]}\n",
    "        condition = 1 if self.__probabilities[clf] else 0\n",
    "        method = results[purpose][condition]\n",
    "        return method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier\n",
    "\n",
    "Perform a simple test using the SuperLearnClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SLC = SuperLearnerClassifier(keep_original=False, probability=True)\n",
    "iris = load_iris()\n",
    "X,y = iris.data, iris.target\n",
    "SLC.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overtrained-Acc\t1.0\n",
      "10xCv-Acc\t0.953333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disagree</th>\n",
       "      <th>Double Fault</th>\n",
       "      <th>Matthews Coef.</th>\n",
       "      <th>Pairs</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941004</td>\n",
       "      <td>(LogisticRegression, DecisionTreeClassifier)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Disagree  Double Fault  Matthews Coef.  \\\n",
       "0      0.04           0.0        0.941004   \n",
       "\n",
       "                                          Pairs   Q  \n",
       "0  (LogisticRegression, DecisionTreeClassifier) NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing_Expertise</th>\n",
       "      <th>Unanimous_Agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Missing_Expertise  Unanimous_Agreement\n",
       "0                0.0                 0.96\n",
       "1                0.0                 0.96"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Estimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy               Estimator\n",
       "0      0.96      LogisticRegression\n",
       "1      1.00  DecisionTreeClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 388 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SLC = SuperLearnerClassifier(keep_original=True, probability=False)\n",
    "iris = load_iris()\n",
    "X,y = iris.data, iris.target\n",
    "SLC.fit(X,y)\n",
    "score = accuracy_score(SLC.predict(X), y)\n",
    "cv_score = np.mean(cross_val_score(SLC, X,y, cv=10))\n",
    "print('Overtrained-Acc',score, sep='\\t')\n",
    "print('10xCv-Acc', cv_score, sep='\\t')\n",
    "display(pd.DataFrame(SLC.diversity(X, y)))\n",
    "display(pd.DataFrame(SLC.coverage(X, y), index=list(range(2))))\n",
    "display(pd.DataFrame(SLC.score_bases(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnitTesting\n",
    "\n",
    "Simple UnitTest class to illustrate SuperLearnerClassifier has all desirable behaviours, features and error handling.\n",
    "The SuperLearnerClassifier uses the Iris dataset to run many different test cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSLC(unittest.TestCase):\n",
    "    '''Unittest class for SuperLearnerClassifier'''\n",
    "    \n",
    "    def use_iris(self):\n",
    "        '''\n",
    "        1. used to load/unpack iris conviniently\n",
    "        '''\n",
    "        iris = load_iris()\n",
    "        return iris.data, iris.target\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_default_init(self):\n",
    "        '''\n",
    "        1. Tests param defaults on instantiation\n",
    "        '''\n",
    "        SLC = SuperLearnerClassifier()\n",
    "        self.assertEqual(SLC.estimators, None)\n",
    "        self.assertEqual(SLC.stacked_estimator, None)\n",
    "        self.assertEqual(SLC.cv, 10)\n",
    "        self.assertEqual(SLC.probability, False)\n",
    "        self.assertEqual(SLC.keep_original, False)\n",
    "        \n",
    "    \n",
    "    def test_default_fit(self):       \n",
    "        '''\n",
    "        1. Base/stacked classifiers are set during fit method\n",
    "        2. Base classifiers match defined defaults\n",
    "        3. Stacked classifier matches defined default\n",
    "        '''\n",
    "        X,y = self.use_iris()\n",
    "        SLC = SuperLearnerClassifier()\n",
    "        SLC.fit(X,y)\n",
    "        default = [LogisticRegression, DecisionTreeClassifier]\n",
    "        for i in range(max(len(default), len(SLC.estimators))):\n",
    "            self.assertTrue(isinstance(SLC.estimators[i], default[i]))\n",
    "        self.assertTrue(isinstance(SLC.stacked_estimator, \n",
    "                                   DecisionTreeClassifier))\n",
    "        \n",
    "        \n",
    "    def test_default_predict(self):\n",
    "        '''\n",
    "        1. predict/predict_proba raise NotFittedError before fitting\n",
    "        2. predict/predict_proba do not raise an error after fitting\n",
    "        '''\n",
    "        X,y = self.use_iris()\n",
    "        SLC = SuperLearnerClassifier()\n",
    "        with self.assertRaises(NotFittedError):\n",
    "            SLC.predict(X)\n",
    "        with self.assertRaises(NotFittedError):\n",
    "            SLC.predict_proba(X)\n",
    "        SLC.fit(X,y)\n",
    "        self.assertEqual(len(X),len(SLC.predict(X)))\n",
    "        self.assertEqual(len(X),len(SLC.predict_proba(X)))\n",
    "\n",
    "        \n",
    "    def test_arg_init(self):\n",
    "        '''\n",
    "        1. Default params modified on instantiation if args are supplied\n",
    "        '''\n",
    "        clfs = [RandomForestClassifier(),\n",
    "                KNeighborsClassifier()]\n",
    "        stacked = LogisticRegression()\n",
    "        cv = 5\n",
    "        probability, keep_original = True, True\n",
    "        SLC = SuperLearnerClassifier(clfs, stacked, cv,\n",
    "                                     probability, keep_original)\n",
    "        self.assertEqual(SLC.estimators, clfs)\n",
    "        self.assertEqual(SLC.stacked_estimator, stacked)\n",
    "        self.assertEqual(SLC.cv, cv)\n",
    "        self.assertEqual(SLC.probability, probability)\n",
    "        self.assertEqual(SLC.keep_original, keep_original)\n",
    "        \n",
    "    \n",
    "    def test_kwarg_init(self):\n",
    "        '''\n",
    "        1. Default params modified on instantiation if kwargs are supplied\n",
    "        '''\n",
    "        clfs = [RandomForestClassifier(), \n",
    "                KNeighborsClassifier()], \n",
    "        stacked = LogisticRegression()\n",
    "        cv = 5\n",
    "        probability, keep_original = True, True\n",
    "        SLC = SuperLearnerClassifier(estimators=clfs, \n",
    "                                     stacked_estimator=stacked, \n",
    "                                     cv=cv,\n",
    "                                     probability=probability, \n",
    "                                     keep_original=keep_original)\n",
    "        self.assertEqual(SLC.estimators, clfs)\n",
    "        self.assertEqual(SLC.stacked_estimator, stacked)\n",
    "        self.assertEqual(SLC.cv, cv)\n",
    "        self.assertEqual(SLC.probability, probability)\n",
    "        self.assertEqual(SLC.keep_original, keep_original)\n",
    "        \n",
    "    \n",
    "    def test_allowed_bases(self):\n",
    "        '''\n",
    "        1. Any allowed base can be used.\n",
    "        2. base is not fitted until fit method is called\n",
    "        3. base is fitted during slc fit method\n",
    "        '''\n",
    "        X,y = self.use_iris()\n",
    "        SLC = SuperLearnerClassifier()\n",
    "        allowed = SLC._SuperLearnerClassifier__allowed_bases\n",
    "        for clf in allowed:\n",
    "            SLC2 = SuperLearnerClassifier(estimators=[clf()])\n",
    "            self.assertTrue(isinstance(SLC2.estimators[0], clf))\n",
    "            SLC2.fit(X,y)\n",
    "            self.assertEqual(len(X),len(SLC2.predict(X)))\n",
    "        \n",
    "            \n",
    "    def test_forbidden_bases(self):\n",
    "        '''\n",
    "        1. forbidden bases raise TypeError during slc fit method.\n",
    "        '''\n",
    "        X,y = self.use_iris()\n",
    "        classifiers = [DecisionTreeRegressor, RandomForestRegressor]\n",
    "        for clf in classifiers:\n",
    "            SLC = SuperLearnerClassifier([clf()])\n",
    "            self.assertTrue(isinstance(SLC.estimators[0], clf))\n",
    "            with self.assertRaises(TypeError):\n",
    "                SLC.fit(X,y)                                                    \n",
    "        classifiers = [clf() for clf in classifiers]\n",
    "        SLC = SuperLearnerClassifier(classifiers)\n",
    "        with self.assertRaises(TypeError):\n",
    "            SLC.fit(X,y)                                                           \n",
    "\n",
    "        \n",
    "    def test_allowed_stacked(self):\n",
    "        '''\n",
    "        1. any allowed stacked estimators can be used.\n",
    "        2. stacked estimator is not fitted until fit method is called.\n",
    "        3. stacked estimator is fitted with slc fit method\n",
    "        '''\n",
    "        SLC = SuperLearnerClassifier()\n",
    "        allowed = SLC._SuperLearnerClassifier__allowed_stacked\n",
    "        for clf in allowed:\n",
    "            SLC2 = SuperLearnerClassifier(stacked_estimator=clf())\n",
    "            self.assertTrue(isinstance(SLC2.stacked_estimator, clf))\n",
    "            SLC2.fit(X,y)\n",
    "            self.assertEqual(len(X),len(SLC2.predict(X)))\n",
    "        \n",
    "            \n",
    "    \n",
    "    def test_forbidden_stacked(self):\n",
    "        '''\n",
    "        1. forbidden stacked estimator raise TypeError during fit method.\n",
    "        '''\n",
    "        X,y = self.use_iris()\n",
    "        SLC = SuperLearnerClassifier(\n",
    "            stacked_estimator=RandomForestRegressor())\n",
    "        self.assertTrue(isinstance(SLC.stacked_estimator, \n",
    "                                   RandomForestRegressor))\n",
    "        with self.assertRaises(TypeError):\n",
    "            SLC.fit(X,y)\n",
    "            \n",
    "            \n",
    "    def test_pandas(self):\n",
    "        '''SLC can handle pandas DataFrames'''\n",
    "        X,y = self.use_iris()\n",
    "        X = pd.DataFrame(X)\n",
    "        SLC = SuperLearnerClassifier()\n",
    "        SLC.fit(X,y)\n",
    "        self.assertEqual(len(X),len(SLC.predict(X)))\n",
    "\n",
    "    \n",
    "    def test_predict_param_grid(self):\n",
    "        '''\n",
    "        1. Validates that a grid of expected parameters pass testing\n",
    "        2. Validates different combinations of program flows.\n",
    "        3. Tuples, lists and ints can be used to specify classifiers\n",
    "        '''\n",
    "        X,y = self.use_iris()\n",
    "        param_grid = {'estimators':[None, \n",
    "                                     [RandomForestClassifier()], \n",
    "                                     (RandomForestClassifier(),),\n",
    "                                     [RandomForestClassifier(), \n",
    "                                          KNeighborsClassifier()],\n",
    "                                     (RandomForestClassifier(),\n",
    "                                          KNeighborsClassifier()),\n",
    "                                     1,2\n",
    "                                    ],\n",
    "                      'stacked_estimator':[None, LogisticRegression()],\n",
    "                      'cv':list(range(2,4)),\n",
    "                      'probability':[True,False],\n",
    "                      'keep_original':[True,False]\n",
    "                     }\n",
    "        all_combos = ParameterGrid(param_grid)\n",
    "        for params in all_combos:\n",
    "            SLC = SuperLearnerClassifier(**params)\n",
    "            with self.assertRaises(NotFittedError):\n",
    "                SLC.predict(X)\n",
    "            with self.assertRaises(NotFittedError):\n",
    "                SLC.predict_proba(X)\n",
    "            SLC.fit(X,y)\n",
    "            self.assertEqual(len(X),len(SLC.predict(X)))\n",
    "            self.assertEqual(len(X),len(SLC.predict_proba(X)))\n",
    "      \n",
    "                \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      ".C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\discriminant_analysis.py:706: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\discriminant_analysis.py:706: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\discriminant_analysis.py:709: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n",
      ".........C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Andy\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 11 tests in 24.085s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data\n",
    "\n",
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing\n",
    "Setup the number of folds for all grid searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.10\n",
    "\n",
    "#Stratified: Preserves class weighting\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Load the dataset and explore it.\n",
    "\n",
    "### Pre-process & Partition Data\n",
    "\n",
    "Perform data pre-processing and manipulation as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'fashion-mnist_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8b195ec4ed0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m            8:\"Bag\",          9:\"Ankle boot\"}\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fashion-mnist_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstratified_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'fashion-mnist_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "def stratified_sample(df, by, frac):\n",
    "    '''takes stratified sample of dataset based on some grouping'''\n",
    "    df = df.groupby(by=[by]).apply(lambda df : df.sample(frac=frac))\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", \n",
    "           2: \"Pullover\",    3:\"Dress\", \n",
    "           4:\"Coat\",         5:\"Sandal\", \n",
    "           6:\"Shirt\",        7:\"Sneaker\", \n",
    "           8:\"Bag\",          9:\"Ankle boot\"}\n",
    "\n",
    "dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "dataset = stratified_sample(dataset, by='label', frac=0.1)\n",
    "X_train = dataset[dataset.columns[1:]]\n",
    "y_train = np.array(dataset[\"label\"])\n",
    "X_train = X_train/255\n",
    "display(X_train.head())\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('fashion-mnist_test.csv')\n",
    "X_test = dataset[dataset.columns[1:]]\n",
    "y_test = np.array(dataset[\"label\"])\n",
    "X_test = X_test/255\n",
    "display(X_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Simple Model\n",
    "\n",
    "Train a Super Learner Classifier using the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Some plane learners. Keeping it simple\n",
    "classifiers = [DecisionTreeClassifier(min_samples_split=20),\n",
    "               RandomForestClassifier(min_samples_split=100),\n",
    "               AdaBoostClassifier(),\n",
    "               ExtraTreesClassifier(min_samples_split=100),\n",
    "               GradientBoostingClassifier(n_estimators=10, min_samples_split=100),\n",
    "               MLPClassifier(),\n",
    "               LogisticRegression(),\n",
    "               KNeighborsClassifier(),\n",
    "               NearestCentroid(),\n",
    "               GaussianNB(),\n",
    "               MultinomialNB(),\n",
    "               LinearDiscriminantAnalysis(),\n",
    "               QuadraticDiscriminantAnalysis()\n",
    "            ]\n",
    "\n",
    "address = os.path.join(super_learners, 'simple_model.pkl' )\n",
    "    \n",
    "try:\n",
    "    clf = joblib.load(address)\n",
    "    print('Loaded...')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('Building...')\n",
    "    clf = SuperLearnerClassifier(estimators = classifiers, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    joblib.dump(clf, address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Warning: SLOW\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy :', accuracy)\n",
    "display(pd.crosstab(np.array(y_test), \n",
    "            y_pred,\n",
    "            rownames=['True'],\n",
    "            colnames=['Predicted'],\n",
    "            margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Warning: SLOW\n",
    "for base in clf.estimators:\n",
    "    start = time()\n",
    "    base.fit(X_train, y_train)\n",
    "    delta_fit = format(time() - start, '.2f')\n",
    "    \n",
    "    start = time()\n",
    "    y_pred = base.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    delta_pred = format(time() - start, '.2f')\n",
    "    \n",
    "    print('Fit_Time', delta_fit ,\n",
    "          '\\tPred_Time :', delta_pred,\n",
    "          '\\tAccuracy :', accuracy,\n",
    "           name(base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Impression\n",
    "\n",
    "#### Train Time Slow :  10 mins\n",
    "This was expected each base estimator must run cross_val_predict() as part of fit method.\n",
    "Smaller levels of CV may would speed up train time, but could cause a steep drop in accuracy if the model is overtrained and hasn't been fitted on enough data. KNNs slow predict time will contribute to this, then models with large train time like GradBoosting, AdaBoost and MLP and LogisticRegression have large train times.\n",
    "\n",
    "#### Accuracy Good: 78+\n",
    "Good for a model that hasn't been tuned. But some individuals in the ensemble perform better than the collective ensemble.\n",
    "Untuned base learners. RandomForest,ExtraTrees (which themselves are ensembles) have similar performance to the SuperLearner.\n",
    "MLP, LogisticRegression both perform noticeably stronger than SuperLearner. LDA, 5NN, had similar or better performance to the SuperLearner but are suprisingly simple but algorithms.\n",
    "A reasonable goal would be to beat an MLP to get 86+.\n",
    "\n",
    "\n",
    "#### Prediction Time Slow: 2.75 mins\n",
    "KNN is the source of this problem. For such a large amount of features and samples, NN computation is expensive. This is the price it pays for not \"generalising\" in its training phase. Since the fit method of the SuperLearnerClassifier requires KNN to make predicitons, KNN is also culpible for slow training.\n",
    "\n",
    "#### Response:\n",
    "GradBoosting was outperformed by two simpler ensembles and majorly contributes to train time.\n",
    "KNN should be removed from the ensemble if it does not significantly contribute to ensemble accuracy. KNN weakens prediction time and has overall a similar peformance to LDA. Other member such as QDA, GaussianNB, ADABoost should be trimmed to improve train/pred times and they most likely weaken the ensemble due to their poor performance.\n",
    "\n",
    "#### Result: \n",
    "    Accuracy increase to 81+\n",
    "    Train time reduced to 3.33mins, \n",
    "    Prediction time is now in accepted range less than 0.5 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(min_samples_split=20),\n",
    "               RandomForestClassifier(min_samples_split=100),\n",
    "               ExtraTreesClassifier(min_samples_split=100),\n",
    "               GradientBoostingClassifier(n_estimators=10, min_samples_split=100),\n",
    "               MLPClassifier(),\n",
    "               LogisticRegression(),\n",
    "               NearestCentroid(),\n",
    "               MultinomialNB(),\n",
    "               LinearDiscriminantAnalysis(),\n",
    "              ]\n",
    "\n",
    "address = os.path.join(super_learners, 'pruned_simple_model.pkl')\n",
    "\n",
    "try:\n",
    "    pruned_clf = joblib.load(address)\n",
    "    print('Loaded...')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    pruned_clf = SuperLearnerClassifier(estimators=classifiers, cv=5)\n",
    "    print('Building...')\n",
    "    pruned_clf.fit(X_train, y_train)\n",
    "    joblib.dump(pruned_clf, address, compress=9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = pruned_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy :', accuracy)\n",
    "display(pd.crosstab(np.array(y_test), \n",
    "            y_pred,\n",
    "            rownames=['True'],\n",
    "            colnames=['Predicted'],\n",
    "            margins=True))\n",
    "\n",
    "display(pd.DataFrame(pruned_clf.diversity(X_test, y_test)))\n",
    "display(pd.DataFrame(pruned_clf.coverage(X_test, y_test), index=list(range(2))))\n",
    "display(pd.DataFrame(pruned_clf.score_bases(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Experiment (Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "accuracys = cross_val_score(pruned_clf, X_train, y_train, cv=10)\n",
    "mean = format(np.mean(accuracys),'.4f')\n",
    "std = format(accuracys.std(), '.4f')\n",
    "print('10xCV Accuracy : ', mean, '+-', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    10XCV Accuracy is similar to accuracy on the test_set; 81%. \n",
    "    STD of 1% so variance is relatively small.\n",
    "    10XCV takes 30 mins, quite expensive way to measure accuracy.\n",
    "    The SuperLearnerClassifer seems to generalise quite well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Stack Layer Approaches (Task 5)\n",
    "\n",
    "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "param_grid = {'estimators':[classifiers],\n",
    "              'probability':[True, False],\n",
    "              'stacked_estimator':[DecisionTreeClassifier(), \n",
    "                                   LogisticRegression() ],\n",
    "              'cv':[5]\n",
    "             }\n",
    "\n",
    "address = os.path.join(searches, 'stack_layer_approaches.pkl')\n",
    "\n",
    "try:\n",
    "    search = joblib.load(address)\n",
    "    print('Loaded...')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('Building...')\n",
    "    search = GridSearchCV(SuperLearnerClassifier(), \n",
    "                      param_grid, \n",
    "                      cv=cv_folds, \n",
    "                      #verbose = 2, \n",
    "                      return_train_score=True,\n",
    "                      refit=True).fit(X_train,y_train)\n",
    "    joblib.dump(search, address, compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "results['stacked'] = results['param_stacked_estimator']\n",
    "info = [ 'mean_test_score',\n",
    "         'std_test_score',\n",
    "         'mean_fit_time',\n",
    "         'std_fit_time',\n",
    "         'mean_score_time',\n",
    "         'std_score_time',\n",
    "         'param_probability',\n",
    "         'stacked']\n",
    "results = results.sort_values('rank_test_score')\n",
    "display(results[info].round(2))\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(search.best_estimator_.diversity(X_test, y_test)))\n",
    "display(pd.DataFrame(search.best_estimator_.coverage(X_test, y_test), index=list(range(2))))\n",
    "display(pd.DataFrame(search.best_estimator_.score_bases(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search.best_estimator_.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy :', accuracy)\n",
    "display(pd.crosstab(np.array(y_test), \n",
    "                    y_pred,\n",
    "                    rownames=['True'],\n",
    "                    colnames=['Predicted'],\n",
    "                    margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Stacked Layer Approaches\n",
    "\n",
    "\n",
    "#### Overview\n",
    "In theory, probability outputs are continuous allowing base learners to contribute their estimated probability distribution function to the stacked layer. The infromation they provide is of higher resolution and should help capture boundary cases. It is not expected to catch any outliers in the data.\n",
    "\n",
    "The from the reasoning above, the expected result is: for the majority of cases, probability based labels would improve the performance on the SuperLearners. This is not the case. Instead, the pairwise interaction between these different hyperparameters appears to be significant. However, the strongest learner had probability based labels. It benefitted from the higher resolution infromation of its ensemble.\n",
    "\n",
    "There are a number of phenomena to be noted from the results. \n",
    "\n",
    "        1. The LogisticRegression stacked layer is enhanced by using probability based labels. \n",
    "           DecisionTree stacked layer is weakened by by using probability based labels.\n",
    "    \n",
    "        2. The magnitude of this change is quite large in both cases; \n",
    "           \n",
    "           LogisticRegression stacked layer appears to be more sensative to this change. \n",
    "           It produced both the best and worst results depending on enabling/disabling the probability parameter.\n",
    "           The difference in accuracy for  is nearly 25%. \n",
    "           \n",
    "           DecisionTrees both perform reasonably well giving them a mid-table performance.\n",
    "           For DecisionTrees this difference is nearly 2% which is still quite big.\n",
    "\n",
    "       3. The pairwise interaction of hyperparameters is quite significant. Probabilities do not always enhance the ensemble.\n",
    "          The use of LogisticRegression vs DecisionTrees in the stacked layer does not always enhance accuracy.\n",
    "          Therefore, it is necessary to perform wide searches of the parameter space using GridSearches and RandomSearches.\n",
    "\n",
    "\n",
    "        4. Use of probability based labels slightly increases prediciton time, in general.\n",
    "           Probability based labels give a negligable increase fit time for LogisticRegression.\n",
    "           Probability based labels noticably increase fit time for decision tree stacked layer.\n",
    "          \n",
    "\n",
    "These observations were taken from the aggregated results above, they are also consitent with the results of each of the 5 splits performed in the data. \n",
    "\n",
    "\n",
    "#### Logistic Regression\n",
    "LogisticRegression appears to be very sensitive to using probability labels in the stacked layer. Giving it strong performance when present and much weaker when not. Consider this could be due to having relatively few features to fit to in the stacked layer, with a larger ensemble LogisticRegression may perform better, however a larger ensemble would negatively impact training time noticably for each member added and diversity becomes increasingly difficult to achieve. Enabling probability achieves the desired effect with minimal training/prediction costs. It is understandable why LogisticRegression performs well in the stacked layer, given probability based labels, it is easy to percieve the certainty different classifiers have of their prediction. Then by setting the coeficients, the predictions of different base classifiers essentially become weighted. Viewing the coeficients of the regression should give insight to which classifiers its favoring, it could be the case that it is simply predicting colinearly with the MLP in all cases. To enhance on this prediction, base classifiers need to be comparably accurate to MLP while guessing differently. \n",
    "\n",
    "#### Decision Tree\n",
    "DecisionTrees appear to be less sensative to this change, but unlike the LogisticRegression stacked layer, it is weakened from using probability based labels. Perhaps it is overtrained from picking up noise in the probability labels; weakening its generalisation accuracy. An interesting experiment would be to tune the stacked estimator for both probability based and class based labels and seeing which gives a stronger performance. The DecisionTree may benefit from keeping_original data, allowing it to favor certain classifiers for certain input data, which could help it use the MLPs power for some data, but capture diversity of other classifiers for different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters (Task 7)\n",
    "\n",
    "Perfrom a grid search experiment to detemrine the optimal architecture and hyper-parameter values for the SuperLearnClasssifier for the MNIST Fashion classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Bases\n",
    "To set up a search space for the SuperLearner first bases will be tuned a small bit to increase the accuracy of each member.\n",
    "The top10 results for each base will be tablulated for human interpretation. The reason for this is to verify that each base classifier is reasonably accuracy and will not hamper fit/prediction time during SupearLearner GridSeach.\n",
    "\n",
    "The best classifiers will be listed and sampled repeatedly and an exhaustive combination of samples will be taken between different ensemble sizes. These samples are passed into the param_grid for a GridSearchCV in the usual fashion.\n",
    "\n",
    "Similarly a stacked estimator search space will be prepared, to be passed into the GridSearch param_grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "base_search_space = {\n",
    "    #consitently strong learners.\n",
    "    LogisticRegression:{'fit_intercept':[True,False],\n",
    "                        'solver':['liblinear',\n",
    "                                 # 'sag' #too costly for minimal accuracy gain.\n",
    "                                 ]},\n",
    "    \n",
    "    #gini learners were observed to be more successful than entropy\n",
    "    RandomForestClassifier:{'n_estimators':list(range(10,101,10)),\n",
    "                            'min_samples_split':list(range(10,80,10)),\n",
    "                            'criterion':['gini',]},\n",
    "    \n",
    "    # Tends to perform weakly\n",
    "    #DecisionTreeClassifier:{'min_samples_split':list(range(2,103,25)),\n",
    "    #                        'criterion':['gini',\n",
    "    #                                     'entropy',\n",
    "    #                                    ]},\n",
    "    \n",
    "    #gini learners were observed to be more successful than entropy.\n",
    "    ExtraTreesClassifier:{'n_estimators':list(range(10,101,10)),\n",
    "                          'min_samples_split':list(range(10,80,10)),\n",
    "                          'criterion':['gini',\n",
    "                                      #'entropy',\n",
    "                                      ]},\n",
    "    \n",
    "    #range tended to produce inexpensive and accurate MLPClassifiers\n",
    "    MLPClassifier:{'hidden_layer_sizes':list(range(200,301,10)),\n",
    "                    'activation':['relu'], #tanh:ok, logistic:weak\n",
    "                   'learning_rate':['adaptive'], #faster when bigger\n",
    "                   'early_stopping':[True]}, #good for time\n",
    "    \n",
    "    #too weak\n",
    "    NearestCentroid:{'shrink_threshold':list(np.arange(0,1,0.2))},\n",
    "    \n",
    "    #too weak\n",
    "    MultinomialNB:{'alpha':list(np.arange(0,1,0.2))},\n",
    "    \n",
    "    #consistenly good baseline.\n",
    "    LinearDiscriminantAnalysis:[{'shrinkage':['auto'],\n",
    "                                 'solver':['lsqr']},\n",
    "                               {'solver':['lsqr','svd']}\n",
    "                               ]                 \n",
    "}\n",
    "\n",
    "'''todo: score using log-loss to optimise probability based stacked layer'''\n",
    "\n",
    "\n",
    "#give this an additional param for naming\n",
    "def make_filepath(class_str):\n",
    "    class_str = str(class_str).split('.')[-1]\n",
    "    alphas = [c for c in list(class_str) if c.isalnum()]\n",
    "    filename = ''.join(alphas+['.pkl'])\n",
    "    path = os.path.join(tuned_bases, filename)\n",
    "    return path\n",
    "\n",
    "try:\n",
    "    results_clfs = [joblib.load(make_filepath(est)) \\\n",
    "                   for est, est_params in base_search_space.items()]\n",
    "    print('Loaded...')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    def search_base(estimator, params):\n",
    "        search = GridSearchCV(estimator=estimator, \n",
    "                              param_grid=params, \n",
    "                              cv=cv_folds, \n",
    "                              #verbose=2, \n",
    "                              n_jobs=-1, \n",
    "                              refit=True,\n",
    "                              return_train_score=True)\n",
    "        best = search.fit(X_train, y_train)\n",
    "        address = make_filepath(str(type(best.best_estimator_)))\n",
    "        joblib.dump(best, address, compress=9)\n",
    "        return best\n",
    "    \n",
    "    print('Building...')\n",
    "    results_clfs = [search_base(est(), est_params) \\\n",
    "                    for est,est_params in base_search_space.items()]\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = [ 'params',\n",
    "         'mean_test_score',\n",
    "         'std_test_score',\n",
    "         'mean_fit_time',\n",
    "         'std_fit_time',\n",
    "         'mean_score_time',\n",
    "         'std_score_time',\n",
    "       ]\n",
    "\n",
    "for search in results_clfs:\n",
    "    df = pd.DataFrame(search.cv_results_).sort_values('rank_test_score')\n",
    "    clf_type = type(search.best_estimator_)\n",
    "    \n",
    "    #Title\n",
    "    display(clf_type)\n",
    "    \n",
    "    #gives clear overview of params used for top 10\n",
    "    display(df[info].round(3).head(10)['params'].values)\n",
    "    \n",
    "    #performance breakdown\n",
    "    display(df[info].round(3).head(10))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test = [result.best_estimator_ for result in results_clfs]\n",
    "test_slc = SuperLearnerClassifier(test, stacked_estimator=LogisticRegression())\n",
    "test_slc.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test_slc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy :', accuracy)\n",
    "display(pd.crosstab(np.array(y_test), \n",
    "                    y_pred,\n",
    "                    rownames=['True'],\n",
    "                    colnames=['Predicted'],\n",
    "                    margins=True))\n",
    "\n",
    "\n",
    "display(pd.DataFrame(test_slc.diversity(X_test, y_test)))\n",
    "display(pd.DataFrame(test_slc.coverage(X_test, y_test), index=list(range(2))))\n",
    "display(pd.DataFrame(test_slc.score_bases(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Classifiers\n",
    "GridSearching base learners can be used not only to maximise accuracy but also to get an idea for the overall performance of classifiers with different sets of hyperparameters, accuracy and speed.\n",
    "\n",
    "It also gives a flavor for which hyperparameters are better suited to this sort of problem. E.g. A liblinear logistic regression fits 5 time faster than a 'saga', with only a 0.1% accuracy difference in accuracy. It is important to not compromise on accuracy too much as members must be strong and predict more difficult examples.\n",
    "\n",
    "\n",
    "### Pruning\n",
    "As was seen in the test run, pruning weaker ensemble members can increase the accuracy of the whole as they can be sources of noise rather than diversity. The models complexity is also reduced making it less expensive to train and predict. By increasing  the number of estimators in Random Forests, and adding more layers to an MLP the models can become more expensive to train which can impede gridsearching. By assessing the top results for each base a classifier can be chosen that are fast and also accurate. After exploring the optimal overall architecture, members can be retuned purely for accuracy.\n",
    "\n",
    "\n",
    "### LinearDiscriminantAnalysis: BaseLine\n",
    "LDA is a very simple and well understood process. Like KNN it is often used as a baseline to compare other classifiers to. In this case KNN is too expensive for prediction, but LDA shows strong results for little expense. If a model cannot outperform LDA, it will not be included in the ensemble. Its accuracy is consistently 80+, so that will serve as the accuracy cut off.\n",
    "\n",
    "### LogisticRegression\n",
    "Consistently strong 83+, using sag and saga tend to be marginally more accuracy by a fraction of a percent but train slowly. Liblinear was used as it negligably weaker but significantly faster.\n",
    "\n",
    "### Decision Trees \n",
    "DecisionTrees had reasonable performnce of around 74%. It however has a weaker performance than random forests and extra trees and despite being ensembles, they train for faster or comparable times. DecisionTrees do not meet the baseline set by LDA, so it is more likely to be a source of noise rather than adding diversity. For this reason, decision trees will be removed.\n",
    "\n",
    "### RandomForests OR ExtraTrees\n",
    "Both tend to score 83+\n",
    "As both are strong learners, both will be kept till the main gridsearch, which will decide if the ensemble is enhanced by the presence of both models or if one should be dropped. It is noted that ExtraTrees tend to fit faster which make them slightly more desirable.\n",
    "\n",
    "### NearestCentroid and MultinomialNB\n",
    "Both perform at roughly 68%. NearestCentroid and MultinomialNB did not improve well with tuning and are still weak learners. They probably add more noise than they do diversity. While they are inexpensive to train, they are probably best removed as they are far from the standard set by LDA.\n",
    "\n",
    "### MLP\n",
    "A strong learner overall, many combinations of hyperparameters were investigated before hand testing different activation functions and learning rate combinations. The list in the param search was refined to produce models that trained quickly and predicted accurately. Tends to be 84+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the list to only certain allowed classifiers, the usual set. use name.\n",
    "allowed = (LogisticRegression,\n",
    "           RandomForestClassifier,\n",
    "           ExtraTreesClassifier,\n",
    "           LinearDiscriminantAnalysis,\n",
    "           MLPClassifier)\n",
    "allowed_bases = [clf for clf in test if isinstance(clf, allowed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range of desired ensemble sizes to explore\n",
    "lower, upper = 4,5\n",
    "\n",
    "best_bases = [base for base in allowed_bases]\n",
    "all_base_combos = list(chain.from_iterable(\n",
    "    [combinations(best_bases,i) for i in range(lower,upper+1)]))\n",
    "stacked_search_space = {LogisticRegression:{\n",
    "                             'fit_intercept':[True,\n",
    "                                              #False\n",
    "                                             ]\n",
    "                        },\n",
    "                        #Too weak, removed from gridsearch after testing\n",
    "                        #DecisionTreeClassifier:{\n",
    "                        #    'min_samples_split':list(range(2,202,50)),\n",
    "                        #    'criterion':['gini',\n",
    "                        #                 'entropy'],\n",
    "                        #}\n",
    "                       }\n",
    "all_stacked_estimators = [[clf(**params) for params in ParameterGrid(space)]\\\n",
    "                         for clf, space in stacked_search_space.items()]\n",
    "all_stacked_estimators = list(chain.from_iterable(all_stacked_estimators))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'estimators':all_base_combos,\n",
    "    'probability' : [True, \n",
    "                     #False\n",
    "                    ],\n",
    "    'keep_original' : [False],\n",
    "    'stacked_estimator' : all_stacked_estimators,\n",
    "    'cv': [#3,tends to be weak (especially with smaller data) \n",
    "          5,\n",
    "          #10, #too long with no significant contribution to scores.\n",
    "          ]\n",
    "}\n",
    "\n",
    "address = os.path.join(searches, 'overall_architectures.pkl')\n",
    "\n",
    "try:\n",
    "    joblib.load(address)\n",
    "    print('Loaded...')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('Building...')\n",
    "    search = GridSearchCV(SuperLearnerClassifier(), \n",
    "                          param_grid, \n",
    "                          cv=cv_folds, \n",
    "                          #verbose = 2, \n",
    "                          return_train_score=True,\n",
    "                          refit=True).fit(X_train,y_train)\n",
    "    joblib.dump(search, address, compress=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(search.cv_results_).sort_values('rank_test_score')    \n",
    "#performance breakdown\n",
    "top_params = df[info].round(4).head(12)['params'].values\n",
    "display(df[info].round(4).head(12))\n",
    "print()\n",
    "\n",
    "print('Ensemble Sizes')\n",
    "display([len(entry['estimators']) for entry in top_params])\n",
    "print()\n",
    "\n",
    "print('Ensemble Params:\\n',top_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Results\n",
    "Top SuperLearners have improved on the accuracys' of their base estimators suggesting that there is some benefit to ensembleing  them. Many results are similar, so it may be worth performing paired t-tests to see if the differences are statistically significant. \n",
    "\n",
    "### CV=5 Vs. CV=10\n",
    "Interestingly at cv=5 tends to perform better on average, but the top model has cv=10 and exclude LDA, which is a very stable classifier that does not require does not require alot of data to fit well. The models using 10xCV tend to need 2-3 times longer to train. But cv=5 is weaker by 0.1-0.2 percent. So as more investigation goes on cv=5 should continue to be used to save time.\n",
    "\n",
    "### Ensemble Sizes\n",
    "The space searched allowed for ensmeble sizes of 5/4. In this case the larger ensemble performed best, probably as each classifier does contribute to the diversity, while being accurate. This size seems reasonable as pruning weaker members did raise the accuracy. A good feature to implement to grow the ensemble is to perform a forward search and add more exotic classifiers from the SKLearn toolkit.\n",
    "\n",
    "In earlier searches with larger ensembles, it seemed that larger ensembles performed mid_pack implying that their is some estimator that should probably be pruned and when that estimator that detracts from the ensemble. However its presence is not as significant as losing an effective ensmeble member. This supported the argument that pruning the weak members was the optimal choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model selected by the grid search on a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#best two models params.\n",
    "top_2 = df[info].round(4).head(10)['params'].values[:2]\n",
    "filenames = ['first.pkl', 'second.pkl']\n",
    "addresses = [os.path.join(super_learners, name) for name in filenames]\n",
    "\n",
    "for i in range(2):\n",
    "    try:\n",
    "        tuned_SLC = joblib.load(addresses[i])\n",
    "        print('Loaded...')\n",
    "    except FileNotFoundError:\n",
    "        print('Building...')\n",
    "        tuned_SLC = SuperLearnerClassifier(**top_2[i])\n",
    "        tuned_SLC.fit(X_train, y_train)\n",
    "        joblib.dump(tuned_SLC, addresses[i], compress=9)\n",
    "    \n",
    "    y_pred = tuned_SLC.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy :', accuracy)\n",
    "    display(pd.crosstab(np.array(y_test), \n",
    "                        y_pred,\n",
    "                        rownames=['True'],\n",
    "                        colnames=['Predicted'],\n",
    "                        margins=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Results\n",
    "Models have a slightly stronger performance on the hold out set. both are very close to 86 and have marginal difference, between the top two gridsearch results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer (Task 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "filenames = ['first_with_original.pkl', 'second_with_original.pkl']\n",
    "addresses = [os.path.join(super_learners, name) for name in filenames]\n",
    "\n",
    "for i in range(2):\n",
    "    try:\n",
    "        new_SLC = joblib.load(addresses[i])\n",
    "    except FileNotFoundError:\n",
    "        top_2[i]['keep_original'] = True\n",
    "        new_SLC = SuperLearnerClassifier(**top_2[i])\n",
    "        new_SLC.fit(X_train, y_train)\n",
    "        joblib.dump(new_SLC, addresses[i], compress=9)\n",
    "\n",
    "top_clfs = [joblib.load(address) for address in addresses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for clf in top_clfs:\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy :', accuracy)\n",
    "    display(pd.crosstab(np.array(y_test), \n",
    "                        y_pred,\n",
    "                        rownames=['True'],\n",
    "                        colnames=['Predicted'],\n",
    "                        margins=True))\n",
    "    \n",
    "    #accuracys = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    #mean = format(np.mean(accuracys),'.4f')\n",
    "    #std = format(accuracys.std(), '.4f')\n",
    "    #print('5xCV Accuracy : ', mean, '+-', std)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Original Descriptive Features\n",
    "\n",
    "Seems to slightly enhance each model. Though on some runs this parameter weakened the models slightly. It seems more likely that these small deviations in accuracy come from the randomness within the algorithm each time a SuperLearner is trained.\n",
    "\n",
    "By dropping original descriptive features; the logistic regression stacked layer recieves outputs that are somewhat clean and simple. The probability outputs from the base classifiers should correlate well with the actual class labels allowing a logistic model to model these easily. In a sense the base classifiers have extracted most of the usable raw infromation already. Using heterogenous ensembles allows classifiers to cancel some noise from eachother that stems from noise from the raw data. By including the raw data it may just add more noisy data, weakening the ensemble slightly.\n",
    "\n",
    "An argument can be made for using a DecisionTree stacked layer that can 'in esscence' favor different classifiers depending on the raw data. At this stage it would be interesting to give the DecisionTree stacked layer another chance to see its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model (Task 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "first, second = top_clfs\n",
    "\n",
    "print('Pairwise Analysis')\n",
    "display(pd.DataFrame(first.diversity(X_test, y_test)).round(2))\n",
    "\n",
    "print('Individual Analysis')\n",
    "display(pd.DataFrame(first.score_bases(X_test, y_test)).round(2))    \n",
    "\n",
    "print('Ensemble Coverage')\n",
    "display(pd.DataFrame(first.coverage(X_test, y_test), index=list(range(2))))\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Vs. Diversity\n",
    "It should be noted firstly that each of these classifiers tends to have significant overlap with another. The four base estimators in the ensemble agree on 73% of the data, without any conflict. The other extreme of this is that there is approximately 8% of data that no member of the ensemble has correctly classified. Hypothetically, if the stacked layer were able to identify which estimator's predictions to trust for each example; theoretically the ensembles accuracy could reach 92%. \n",
    "\n",
    "Each base is an effective estimator for the hold-out-dataset, all accuracys are above 80. While this is a desirable trait, it may come at the expense of producing models that are highly correlated and show little disagreement. For this reason there is only marginal improvement in the accuracy of the ensemble versus its strongest member. Ideally, another learner can be added that can learn to predict data with missing expertise. Boosting could help in that case, to focus on the tougher 8% of data, decreasing the Missing Expertise and by extension lowering the mean Double Fault measure and encouraging more disagreement.\n",
    "\n",
    "### RandomForests and ExtraTrees\n",
    "It can be seen through pairwise comparison that the RandomForestClassifier and ExtraTreesClassifier are the most similar. This makes sense they are both forests of DecisionTrees with similar hyperparmeters. The difference between the two algorithms is minimal, hence they show little disagreement, high correlations and despite being the two \"strongest\" members, betweem them they have the least combined expertise. This manifests itself through high double faults, low disagreement and large q_statistics and matthews coeficients. \n",
    "\n",
    "### LogisticRegression and RandomForest\n",
    "The best pair is LogisticRegression and RandomForest, their Double Fault metric is only about 10%. This shows there is greater overall covverage. Using two disimilar algorithms allows for increased disagreement, which can lower the DoubleFault measure, resulting in greater overall expertise.\n",
    "\n",
    "### Q Stat\n",
    "According to Kuncheva et al (2003), classifiers that tend to guess colinearly will have high q statistics, by extension an ensemble with a high mean pairwise q statistic is not optimised to have broad expertise. Classifiers that tend to commit many errors against one another will have negative Q statistics, at that point, broad expertise tends to just be noise. Ideally the q statistic is zero at which point classifiers are independent and estimators are optimally diverse. This indicates that when searching through different sets of base classifiers, optimising the mean Q stat of the ensemble should produce ensembles that are sufficiently accurate and diverse.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
